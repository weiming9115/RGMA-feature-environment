{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05b8b27-3c63-49fb-83c4-27fe682a03ee",
   "metadata": {},
   "source": [
    "#### Prototype of the unified faeture-environment extraction with a fixed box size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d35a8-4ad3-415b-bb86-c689189c4a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import cartopy.crs as ccrs\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ae941-7c34-4bab-89a9-1b6e2ec1a8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13baa483-e890-4bc8-84a3-e9efb2d8a16e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coordinates_processors(data):\n",
    "    \"\"\" \n",
    "    converting longitude/latitude into lon/lat \n",
    "    \"\"\"\n",
    "\n",
    "    coord_names = []\n",
    "    for coord_name in data.coords:\n",
    "        coord_names.append(coord_name)\n",
    "\n",
    "    if (set(coord_names) & set(['longitude','latitude'])): # if coordinates set this way...\n",
    "\n",
    "        data2 = data.rename({'latitude': 'lat'})\n",
    "        data2 = data2.rename({'longitude': 'lon'})\n",
    "    else:\n",
    "        data2 = data\n",
    "\n",
    "    # check if latitutde is decreasing\n",
    "    if (data2.lat[1] - data2.lat[0]) < 0:\n",
    "        data2 = data2.reindex(lat=list(reversed(data2.lat))) # flipping latitude accoordingly\n",
    "\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f9804-1708-40cb-8bea-ae8d07d1bd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ds_feature_environment:\n",
    "    \n",
    "    __version__ = \"1.0beta\"\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.name = None                          # name of the feature-environment dataset\n",
    "        self.track_data = None                    # xarray dataset\n",
    "        self.object_data = None                   # xarray dataset\n",
    "        self.env_data = None                      # xarray dataset\n",
    "        self.feature_data_sources = None          # e.g., ERA5, GPM-IMERG+MERGE-IR\n",
    "        self.environmental_data_sources = None    # e.g., ERA5\n",
    "        self.track_frequency = None               # hourly\n",
    "        self.env_frequency = None                 # hourly\n",
    "        self.lon_env = None                       # longitude of the env. data\n",
    "        self.lat_env = None                       # latitude of the env. data\n",
    "        self.lon_feature = None                   # longitude of the feature data\n",
    "        self.lat_feature = None                   # latitude of the feature data\n",
    "        self.feature_track = None\n",
    "        self.feature_mask = None\n",
    "        self.track_dir = None\n",
    "        self.env2d_dir = None\n",
    "        self.env3d_dir = None\n",
    "        self.envderive_dir = None\n",
    "        \n",
    "    def create_featenv_directory(self, path_dir):\n",
    "        \"\"\"\n",
    "        create subdirectories under the given path_dir\n",
    "        \"\"\"\n",
    "\n",
    "        if path_dir.exists():\n",
    "            print('the given directory already exists ...')\n",
    "            main_dir = Path(path_dir)\n",
    "            self.track_dir = Path( str(main_dir) + '/feature_catalogs/track' )\n",
    "            self.env2d_dir = Path( str(main_dir) + '/environment_catalogs/VARS_2D' )\n",
    "            self.env3d_dir = Path( str(main_dir) + '/environment_catalogs/VARS_3D' )\n",
    "            self.envderive_dir = Path( str(main_dir) + '/environment_catalogs/VARS_derived' )\n",
    "        else:\n",
    "            print('generate feature-environment data directory...')\n",
    "\n",
    "            main_dir = Path(path_dir)\n",
    "            featcats_dir = main_dir / 'feature_catalogs'\n",
    "            envcats_dir = main_dir / 'environment_catalogs'\n",
    "            feattrack_dir = featcats_dir / 'track'\n",
    "            featobj_dir = featcats_dir / 'object'\n",
    "            env2d_dir = envcats_dir / 'VARS_2D'\n",
    "            env3d_dir = envcats_dir / 'VARS_3D'\n",
    "            envderive_dir = envcats_dir / 'VARS_derived'\n",
    "\n",
    "            os.system('mkdir {}'.format(main_dir))\n",
    "            os.system('mkdir {}'.format(featcats_dir))\n",
    "            os.system('mkdir {}'.format(envcats_dir))\n",
    "            print('Create main directoy: {}'.format(main_dir))\n",
    "            print('{}'.format(featcats_dir))\n",
    "            print('{}'.format(envcats_dir))\n",
    "\n",
    "            if self.feature_track:\n",
    "                os.system('mkdir {}'.format(feattrack_dir))\n",
    "                print(feattrack_dir)\n",
    "                self.track_dir = feattrack_dir\n",
    "                if self.feature_mask:\n",
    "                    os.system('mkdir {}'.format(feattrack_dir/'2D_mask'))\n",
    "                    print(feattrack_dir/'2D_mask')\n",
    "                    self.featmask_dir = feattrack_dir/'2D_mask'\n",
    "\n",
    "            else:\n",
    "                os.system('mkdir {}'.format(featobj_dir))\n",
    "                print(featobj_dir)\n",
    "                if self.feature_mask:\n",
    "                    os.system('mkdir {}'.format(featobj_dir/'2D_mask'))\n",
    "                    print(featobj_dir/'2D_mask')\n",
    "                    self.featmask_dir = featobj_dir/'2D_mask'\n",
    "\n",
    "            os.system('mkdir {}'.format(env2d_dir))\n",
    "            print(env2d_dir)\n",
    "            self.env2d_dir = env2d_dir\n",
    "\n",
    "            os.system('mkdir {}'.format(envderive_dir))\n",
    "            print(envderive_dir)\n",
    "            self.envderive_dir = envderive_dir\n",
    "\n",
    "            os.system('mkdir {}'.format(env3d_dir))\n",
    "            print(env3d_dir)\n",
    "            self.env3d_dir = env3d_dir\n",
    "    \n",
    "    def load_track_data(self, file_path):\n",
    "        self.track_data = xr.open_dataset(file_path)\n",
    "\n",
    "        return self.track_data\n",
    "\n",
    "    def load_object_data(self, file_path):\n",
    "        self.object_data = xr.open_dataset(file_path)\n",
    "\n",
    "        return self.track_data\n",
    "\n",
    "    def locate_env_data(self, variable_name, path_dir):\n",
    "        self.locate_env_data = {}\n",
    "\n",
    "        if len(self.locate_env_data) == 0:\n",
    "            self.locate_env_data[variable_name] = path_dir\n",
    "\n",
    "    def locate_feature_data(self, variable_name, path_dir):\n",
    "        self.locate_feature_data = {}\n",
    "\n",
    "        if len(self.locate_feature_data) == 0:\n",
    "            self.locate_feature_data[variable_name] = path_dir\n",
    "\n",
    "    def get_track_info(self, track_number):\n",
    "\n",
    "        track_info = self.track_data.sel(tracks=track_number)\n",
    "\n",
    "        return track_info\n",
    "\n",
    "    def get_object_info(self, object_id):\n",
    "\n",
    "        obj_info = self.object_data.sel(object_id=object_id)\n",
    "\n",
    "        return obj_info\n",
    "    \n",
    "    def get_environment_vars_track(self, track_id, lat_range, lon_range, p_level=None):\n",
    "        \n",
    "        if len(self.locate_env_data) == 0:\n",
    "            raise ValueError(\"No environmental data located. Please call locate_env_data() first\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            track_info = self.get_track_info(track_number=track_id)\n",
    "             \n",
    "            lat_cen = track_info.meanlat.values # MCS lat centroid\n",
    "            lon_cen = track_info.meanlon\n",
    "            lon_cen = lon_cen.where(lon_cen >= 0, lon_cen+360) # converting to 0-360\n",
    "            lon_cen = lon_cen.values\n",
    "            \n",
    "            # find out when the tracked MCS ends as indicated by NaT\n",
    "            idx_end = np.where(np.isnat(track_info.base_time.values))[0][0] \n",
    "\n",
    "            data_chunk = []\n",
    "            time_chunk = []\n",
    "            \n",
    "            for t in range(idx_end):\n",
    "\n",
    "                time64 = track_info.base_time[t].values\n",
    "                timestamp = (time64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n",
    "                time_sel = datetime.utcfromtimestamp(timestamp)\n",
    "            \n",
    "                # determine the env_data to be loaded            \n",
    "                year = str(time_sel.year)\n",
    "                month = str(time_sel.month).zfill(2)\n",
    "                day = str(time_sel.day).zfill(2)\n",
    "                hour = str(time_sel.hour).zfill(2)\n",
    "\n",
    "                data_var = []\n",
    "                for var in [i for i in self.locate_env_data.keys()]:\n",
    "                    \n",
    "                    filename = Path('/neelin2020/ERA-5/NC_FILES') /'{}'.format(year)/ 'era-5.{}.{}.{}.nc'.format(var,year,month)\n",
    "                    data_file = xr.open_dataset(filename)\n",
    "                    data_file = coordinates_processors(data_file)\n",
    "                    \n",
    "                    # find nearest ERA5 grid for the MCS centroid\n",
    "                    idx_sel = np.argmin(np.abs(data_file.lon.values - lon_cen[t]))\n",
    "                    lon_cen_reset = data_file.lon[idx_sel]\n",
    "                    idx_sel = np.argmin(np.abs(data_file.lat.values - lat_cen[t]))\n",
    "                    lat_cen_reset = data_file.lat[idx_sel]\n",
    "                \n",
    "                    lat_min = lat_cen_reset - lat_range/2\n",
    "                    lat_max = lat_cen_reset + lat_range/2\n",
    "                    lon_min = lon_cen_reset - lon_range/2\n",
    "                    lon_max = lon_cen_reset + lon_range/2\n",
    "                \n",
    "                    data_extract = data_file.sel(lat=slice(lat_min, lat_max),\n",
    "                                                        lon=slice(lon_min, lon_max))\n",
    "                    data_extract = data_extract.sel(time=time_sel, method='nearest')\n",
    "                    \n",
    "                    # x-y grid poiints coordinate not lat-lon \n",
    "                    dlon = (data_file.lon[1] - data_file.lon[0]).values\n",
    "                    dlat = (data_file.lat[1] - data_file.lat[0]).values\n",
    "                    data_extract_xy = data_extract.interp(lon=np.linspace(data_extract.lon.min(), data_extract.lon.max(),int(lon_range/dlon)+1),\n",
    "                                              lat=np.linspace(data_extract.lat.min(), data_extract.lat.max(),int(lat_range/dlat)+1))\n",
    "                    # converting lat-lon into x-y coordinates\n",
    "                    data_extract_xy = data_extract_xy.assign_coords(x=(\"lon\", np.arange(len(data_extract_xy.lon))), y=(\"lat\", np.arange(len(data_extract_xy.lat))))\n",
    "                    data_extract_xy = data_extract_xy.swap_dims({'lon':'x', 'lat': 'y'}).drop('time')\n",
    "\n",
    "                    if p_level is not None: # for 3-D data ERA5 only, with vertical dim. named \"level\"\n",
    "\n",
    "                        data_extract_xy = data_extract_xy.sel(level=p_level) # update data_extract which is single layer\n",
    "                            \n",
    "                    data_var.append(data_extract_xy)\n",
    "                data_var_merged = xr.merge(data_var) # merge variables into one xr.dataset\n",
    "                data_chunk.append(data_var_merged)\n",
    "                time_chunk.append(time_sel)\n",
    "            \n",
    "            # add base_time into the dataset\n",
    "            data_chunk_xr = xr.concat(data_chunk, dim=pd.Index(range(len(data_chunk)),name='time'))\n",
    "            ds_basetime_xr = xr.Dataset(data_vars=dict(base_time = (['time'], time_chunk)),\n",
    "                                     coords=dict(time = (['time'], range(len(data_chunk)))))\n",
    "            data_track_xr = xr.merge([data_chunk_xr, ds_basetime_xr])\n",
    "            \n",
    "            # save lat/lon into self\n",
    "            self.lon_env = data_file.lon\n",
    "            self.lat_env = data_file.lat\n",
    "                                   \n",
    "            return data_track_xr \n",
    "        \n",
    "    def get_environment_vars_single(self, object_id, lat_range, lon_range, p_level=None):\n",
    "        \n",
    "        if len(self.locate_env_data) == 0:\n",
    "            raise ValueError(\"No environmental data located. Please call locate_env_data() first\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            obj_info = self.get_object_info(object_id=object_id)\n",
    "        \n",
    "            lat_cen = obj_info.meanlat.values # MCS lat centroid\n",
    "            lon_cen = obj_info.meanlon\n",
    "            lon_cen = lon_cen.where(lon_cen >= 0, lon_cen+360) # converting to 0-360\n",
    "            lon_cen = lon_cen.values\n",
    "\n",
    "            data_chunk = []\n",
    "                        \n",
    "            time64 = obj_info.base_time.values\n",
    "            timestamp = (time64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n",
    "            time_sel = datetime.utcfromtimestamp(timestamp)\n",
    "\n",
    "            # determine the env_data to be loaded            \n",
    "            year = str(time_sel.year)\n",
    "            month = str(time_sel.month).zfill(2)\n",
    "            day = str(time_sel.day).zfill(2)\n",
    "            hour = str(time_sel.hour).zfill(2)\n",
    "\n",
    "            data_var = []\n",
    "            for var in [i for i in self.locate_env_data.keys()]:\n",
    "\n",
    "                filename = Path('/neelin2020/ERA-5/NC_FILES') /'{}'.format(year)/ 'era-5.{}.{}.{}.nc'.format(var,year,month)\n",
    "                data_file = xr.open_dataset(filename)\n",
    "                data_file = coordinates_processors(data_file)\n",
    "\n",
    "                # find nearest ERA5 grid for the MCS centroid\n",
    "                idx_sel = np.argmin(np.abs(data_file.lon.values - lon_cen))\n",
    "                lon_cen_reset = data_file.lon[idx_sel]\n",
    "                idx_sel = np.argmin(np.abs(data_file.lat.values - lat_cen))\n",
    "                lat_cen_reset = data_file.lat[idx_sel]\n",
    "\n",
    "                lat_min = lat_cen_reset - lat_range/2\n",
    "                lat_max = lat_cen_reset + lat_range/2\n",
    "                lon_min = lon_cen_reset - lon_range/2\n",
    "                lon_max = lon_cen_reset + lon_range/2\n",
    "\n",
    "                data_extract = data_file.sel(lat=slice(lat_min, lat_max),\n",
    "                                                    lon=slice(lon_min, lon_max))\n",
    "                data_extract = data_extract.sel(time=time_sel, method='nearest')\n",
    "\n",
    "                # x-y grid poiints coordinate not lat-lon \n",
    "                dlon = (data_file.lon[1] - data_file.lon[0]).values\n",
    "                dlat = (data_file.lat[1] - data_file.lat[0]).values\n",
    "                data_extract_xy = data_extract.interp(lon=np.linspace(data_extract.lon.min(), data_extract.lon.max(),int(lon_range/dlon)+1),\n",
    "                                          lat=np.linspace(data_extract.lat.min(), data_extract.lat.max(),int(lat_range/dlat)+1))\n",
    "                # converting lat-lon into x-y coordinates\n",
    "                data_extract_xy = data_extract_xy.assign_coords(x=(\"lon\", np.arange(len(data_extract_xy.lon))), y=(\"lat\", np.arange(len(data_extract_xy.lat))))\n",
    "                data_extract_xy = data_extract_xy.swap_dims({'lon':'x', 'lat': 'y'}).drop('time')\n",
    "\n",
    "                if p_level is not None: # for 3-D data ERA5 only, with vertical dim. named \"level\"\n",
    "\n",
    "                    data_extract_xy = data_extract_xy.sel(level=p_level) # update data_extract which is single layer\n",
    "\n",
    "                data_var.append(data_extract_xy)\n",
    "            data_var_merged = xr.merge(data_var) # merge variables into one xr.dataset\n",
    "                                   \n",
    "        return data_var_merged\n",
    "        \n",
    "    def get_feature_vars_track(self, track_id, lat_range, lon_range):\n",
    "        \n",
    "        if len(self.locate_feature_data) == 0:\n",
    "            raise ValueError(\"No feature data located. Please call locate_feature_data() first\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            track_info = self.get_track_info(track_number=track_id)\n",
    "             \n",
    "            lat_cen = track_info.meanlat.values # MCS lat centroid\n",
    "            lon_cen = track_info.meanlon\n",
    "            lon_cen = lon_cen.where(lon_cen >= 0, lon_cen+360) # converting to 0-360\n",
    "            lon_cen = lon_cen.values\n",
    "            \n",
    "            # find out when the tracked MCS ends as indicated by NaT\n",
    "            idx_end = np.where(np.isnat(track_info.base_time.values))[0][0] \n",
    "\n",
    "            data_chunk = []\n",
    "            time_chunk = []\n",
    "            \n",
    "            for t in range(idx_end):\n",
    "\n",
    "                time64 = track_info.base_time[t].values\n",
    "                timestamp = (time64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n",
    "                time_sel = datetime.utcfromtimestamp(timestamp)\n",
    "            \n",
    "                # determine the env_data to be loaded            \n",
    "                year = str(time_sel.year)\n",
    "                month = str(time_sel.month).zfill(2)\n",
    "                day = str(time_sel.day).zfill(2)\n",
    "                hour = str(time_sel.hour).zfill(2)\n",
    "\n",
    "                data_var = []\n",
    "                for var in [i for i in self.locate_feature_data.keys()]:\n",
    "                    \n",
    "                    filename = Path('/neelin2020/mcs_flextrkr/') /'{}0101.0000_{}0101.0000'.format(year,int(year)+1)/ 'mcstrack_{}{}{}_{}30.nc'.format(year,month,day,hour)\n",
    "                    data_file = xr.open_dataset(filename)[var] # get the specified variable\n",
    "                    data_file = coordinates_processors(data_file)\n",
    "                    # regrid feature grids into the env. data if needed \n",
    "                    data_file = data_file.interp(lon=self.lon_env, lat=self.lat_env)\n",
    "                    \n",
    "                    # find nearest ERA5 grid for the MCS centroid\n",
    "                    idx_sel = np.argmin(np.abs(data_file.lon.values - lon_cen[t]))\n",
    "                    lon_cen_reset = data_file.lon[idx_sel]\n",
    "                    idx_sel = np.argmin(np.abs(data_file.lat.values - lat_cen[t]))\n",
    "                    lat_cen_reset = data_file.lat[idx_sel]\n",
    "                \n",
    "                    lat_min = lat_cen_reset - lat_range/2\n",
    "                    lat_max = lat_cen_reset + lat_range/2\n",
    "                    lon_min = lon_cen_reset - lon_range/2\n",
    "                    lon_max = lon_cen_reset + lon_range/2\n",
    "                \n",
    "                    data_extract = data_file.sel(lat=slice(lat_min, lat_max),\n",
    "                                                        lon=slice(lon_min, lon_max))\n",
    "                    data_extract = data_extract.sel(time=time_sel, method='nearest')\n",
    "                    \n",
    "                    # x-y grid poiints coordinate not lat-lon \n",
    "                    dlon = (data_file.lon[1] - data_file.lon[0]).values\n",
    "                    dlat = (data_file.lat[1] - data_file.lat[0]).values\n",
    "                    data_extract_xy = data_extract.interp(lon=np.linspace(data_extract.lon.min(), data_extract.lon.max(),int(lon_range/dlon)+1),\n",
    "                                              lat=np.linspace(data_extract.lat.min(), data_extract.lat.max(),int(lat_range/dlat)+1))\n",
    "                    # converting lat-lon into x-y coordinates\n",
    "                    data_extract_xy = data_extract_xy.assign_coords(x=(\"lon\", np.arange(len(data_extract_xy.lon))), y=(\"lat\", np.arange(len(data_extract_xy.lat))))\n",
    "                    data_extract_xy = data_extract_xy.swap_dims({'lon':'x', 'lat': 'y'}).drop('time')\n",
    "                    data_var.append(data_extract_xy)\n",
    "                    \n",
    "                data_var_merged = xr.merge(data_var) # merge variables into one xr.dataset\n",
    "                data_chunk.append(data_var_merged)\n",
    "                time_chunk.append(time_sel)\n",
    "            \n",
    "            # add base_time into the dataset\n",
    "            data_chunk_xr = xr.concat(data_chunk, dim=pd.Index(range(len(data_chunk)),name='time'))\n",
    "            ds_basetime_xr = xr.Dataset(data_vars=dict(base_time = (['time'], time_chunk)),\n",
    "                                     coords=dict(time = (['time'], range(len(data_chunk)))))\n",
    "            data_track_xr = xr.merge([data_chunk_xr, ds_basetime_xr], compat='override')\n",
    "            \n",
    "            # save lat/lon into self\n",
    "            self.lon_feature = data_file.lon\n",
    "            self.lat_feature = data_file.lat\n",
    "                                   \n",
    "            return data_track_xr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87899a-0c94-4936-a4f2-2deb157c1820",
   "metadata": {},
   "source": [
    "## Application \n",
    "- Paths of feature track and environmental variables\n",
    "- Call module and save the dataset\n",
    "- Plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ed53e-f143-4f39-8000-5f33122541c8",
   "metadata": {},
   "source": [
    "#### Call feature-environment module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8d809-7c2e-4e5d-9837-79ae7d7e357d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call the feature-environemnt module\n",
    "featenv = ds_feature_environment()\n",
    "print('version: ', featenv.__version__)\n",
    "featenv.name = 'MCS_FLEXTRKR'\n",
    "featenv.feature_data_sources = 'GPM-IMERG; MERGE-IR'\n",
    "featenv.environmental_data_sources = 'ERA5'\n",
    "featenv.track_frequency = 'hourly'\n",
    "featenv.env_frequency = 'hourly'\n",
    "featenv.feature_track = True\n",
    "featenv.feature_mask = True\n",
    "\n",
    "# create directories according to the above descriptions \n",
    "main_dir = Path('/scratch/wmtsai/featenv_test/{}'.format(featenv.name))\n",
    "featenv.create_featenv_directory(main_dir)\n",
    "\n",
    "print(\"Feature data sources:\", featenv.feature_data_sources)\n",
    "print(\"Environmental data sources:\", featenv.environmental_data_sources)\n",
    "\n",
    "# 1. locate environment variables: variable names, direct paths\n",
    "env_dir = Path('/neelin2020/ERA-5/NC_FILES/')\n",
    "feat_dir = Path('/neelin2020/mcs_flextrkr/')\n",
    "featenv.locate_env_data('T', env_dir)\n",
    "featenv.locate_env_data.update({'q': env_dir})\n",
    "featenv.locate_env_data.update({'ua': env_dir})\n",
    "featenv.locate_env_data.update({'va': env_dir})\n",
    "featenv.locate_env_data.update({'omega': env_dir})\n",
    "\n",
    "featenv.locate_feature_data('cloudtracknumber_nomergesplit', feat_dir)\n",
    "featenv.locate_feature_data.update({'precipitation': feat_dir})\n",
    "featenv.locate_feature_data.update({'tb': feat_dir})\n",
    "\n",
    "print('Environmental data located: \\n',featenv.locate_env_data)\n",
    "print('Feature data located: \\n',featenv.locate_feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c9369-eb87-46e7-8e32-093b32702856",
   "metadata": {},
   "source": [
    "#### 1. feature tracks: MCS tracks as example \n",
    "(timestamps for each track, lat_centroid, lon_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee392e-4462-4163-8995-76a46fa1eb7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load feature track : global MCS tracks from FLEXTRKR in 2020 as example\n",
    "processed_year = 2014\n",
    "track_dir = Path('/neelin2020/mcs_flextrkr/mcs_stats/')\n",
    "track_data = featenv.load_track_data(track_dir / 'mcs_tracks_final_extc_{}0101.0000_{}0101.0000.nc'.format(processed_year, processed_year+1))\n",
    "\n",
    "# 2. a subset of MCSs over the tropical Indian Ocean (50-90, -10,10)\n",
    "meanlon = track_data.meanlon.sel(times=0)\n",
    "meanlat = track_data.meanlat.sel(times=0)\n",
    "cond1 = (meanlon >= 50) & (meanlon <=90)\n",
    "cond2 = (meanlat >= -10) & (meanlat <=10)\n",
    "track_sub = np.intersect1d(np.where(cond1 == 1)[0], np.where(cond2 == 1)[0])\n",
    "\n",
    "# update track_data with a small subset\n",
    "featenv.track_data = track_data.isel(tracks=track_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca8ec1c-0f81-42a0-80d9-1cd1df579b71",
   "metadata": {},
   "source": [
    "#### 2. \"get_environment_vars\" and return the individual feature-env data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6397349-7d74-41f7-9392-c3805e882780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# extract feat-env data for individual tracks\n",
    "ds_merged = []\n",
    "for track in featenv.track_data.tracks.values[:3]: # take the first 100 tracks as example\n",
    "    ds_env_vars = featenv.get_environment_vars_track(track_id=track, lat_range=10, lon_range=10)\n",
    "    ds_feat_vars = featenv.get_feature_vars_track(track_id=track, lat_range=10, lon_range=10)\n",
    "    ds_vars = xr.merge([ds_env_vars, ds_feat_vars], compat='override') # some float differeces. TBD\n",
    "    ds_merged.append(ds_vars)\n",
    "ds_merged_xr = xr.concat(ds_merged, dim=pd.Index(featenv.track_data.tracks.values[:3], name='tracks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb306652-8dbd-4ba1-aa3c-294beea4767d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Save datasets of individual feature tracks/feature objects \n",
    "- create directories of feature catelogs and environmental varaibles\n",
    "- feature track in the standard format (time, lat_centroid, lon_centroid)\n",
    "- environmental variables: 2-D / 3-D and subdirectories named by the varialbe short names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672d79d-5db2-4c5b-9798-1067a76092c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# save feature and environmental variables accordingly\n",
    "for var in ds_merged_xr.keys():\n",
    "    \n",
    "    if var != 'base_time':\n",
    "        ds = ds_merged_xr[var]\n",
    "        check3d = [i for i in ds.dims if i == 'level']\n",
    "        if check3d and len(ds.dims) > 2:\n",
    "            out_dir = featenv.env3d_dir\n",
    "        elif len(ds.dims) > 2:\n",
    "            out_dir = featenv.env2d_dir\n",
    "    \n",
    "        print(out_dir)\n",
    "        ds.to_netcdf(out_dir / '{}_{}_merged.nc'.format(featenv.name, var), encoding={var: {'dtype': 'float32'}})\n",
    "        print('save file: {}_{}_merged.nc'.format(featenv.name, var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22bb832-4aaa-4cff-b290-d8a2569a9d3f",
   "metadata": {},
   "source": [
    "#### 4. Simple demonstration of data capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68157358-5b17-4752-a223-b109f8336b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sample = xr.open_dataset(featenv.env3d_dir / 'MCS_FLEXTRKR_q_merged.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f18d95-6a6e-4e57-b7c3-6f1e65705e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416072f-2430-4b1b-9ba0-9a3fe0dc405a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cwv = 100/9.8*data_sample.q.integrate('level')\n",
    "cwv_composite = cwv.mean(('x','y'))\n",
    "cwv_composite.plot(cmap='terrain_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81c92f-d218-46c0-a659-34a2269bcca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38f19a51-d3e6-4f37-82c6-d427bc28110d",
   "metadata": {},
   "source": [
    "#### 5. add environmental variables\n",
    "- In addition to standard outputs, users can use the module function to extract varaibles from external data sources (gridded data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc103b7-ff72-4694-b10d-0a41920fd0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d62026-8f85-44ed-b20f-4de7c63d08f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a0bef-1748-419f-93c7-2df4076bea65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf6f540-d6a3-4e87-a583-954045899395",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### II. feature objects: time, lat_centroid, lon_centroid\n",
    "- e.g., co-occurring features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734df6b-7561-4a7d-ad0e-63ed64bb5692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating demo dataset (id, time, lat_centroid, lon_centroid)\n",
    "obj_id = np.array([1,2])\n",
    "base_time = np.array([datetime(2020,1,1,0),datetime(2020,1,1,0)])\n",
    "meanlat = np.array([13.5, -10]) # lat of feature centroid\n",
    "meanlon = np.array([50, 160])\n",
    "\n",
    "feature_obj = xr.Dataset(data_vars=dict(\n",
    "                         base_time=(['object_id'], base_time),\n",
    "                         meanlon=(['object_id'], meanlon),    \n",
    "                         meanlat=(['object_id'], meanlat)),\n",
    "                         coords=dict(object_id = (['object_id'], obj_id)))\n",
    "\n",
    "featenv.object_data = feature_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f73a37-8a46-4490-be8e-0dfbfd393d4a",
   "metadata": {},
   "source": [
    "#### use \"get_environment_vars\" and return the individual feature-env data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40612ffc-6284-48bb-ab81-10d0b735a542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract feat-env data for a single object\n",
    "ds_obj_out = featenv.get_environment_vars_single(object_id=1, lat_range=10, lon_range=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc55a87-d9b7-4ba1-ac89-b114494ec76d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(5,4))\n",
    "cp = ax.pcolormesh(ds_obj_out.lon, ds_obj_out.lat, ds_obj_out.sel(level=1000).q)\n",
    "plt.colorbar(cp)\n",
    "ax.set_title(ds_obj_out.time.values)\n",
    "ax.grid(ls=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e17e92-cd60-4eeb-9b7f-0a1155711f79",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Save datasets of individual feature tracks/feature objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df4b70-7aff-43eb-b572-43c2f1cade28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_dir = Path('/scratch/wmtsai/test_ground/')\n",
    "ds_track_out.to_netcdf(out_dir / 'featenv_dataset.test.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80bf9bb-ec8c-45f6-8db1-c12b7fbb2573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "featenv.track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74060b-d7ce-4a6b-8be6-4ce4031cff6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_kernel",
   "language": "python",
   "name": "base_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
